{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33e1960a-757e-4bd3-8081-5bed38121f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution x1 = -1.110667, cost = -3.246394, after 11 iterations\n",
      "Solution x2 = -1.110341, cost = -3.246394, after 29 iterations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nHàm grad(x): Tính gradient (đạo hàm) của hàm mục tiêu. Hàm trả về kết quả theo công thức 2*x + 5*np.cos(x).\\n\\nHàm cost(x): Tính giá trị của hàm mục tiêu với công thức x**2 + 5*np.sin(x). Đây là hàm để tối ưu hóa.\\n\\nHàm myGD(x0, eta):\\n\\nx0: Giá trị khởi tạo của biến.\\neta: Tốc độ học (learning rate) bước nhảy trong mỗi lần lặp.\\nHàm thực hiện tối ưu hóa dựa trên Gradient Descent (GD). Nó lặp qua tối đa 100 lần, tính cập nhật giá trị mới của x bằng cách trừ đi gradient đã nhân với tốc độ học. Nếu gradient nhỏ hơn một ngưỡng rất nhỏ (1e-3), vòng lặp sẽ dừng lại.\\nKết quả là danh sách các giá trị x đã tìm thấy và số lần lặp (iteration).\\nPhần kết quả: In ra nghiệm tìm được của hàm x1 và x2, cùng với giá trị hàm chi phí và số lần lặp cho mỗi nghiệm.\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def grad(x):\n",
    "    return 2*x + 5*np.cos(x)\n",
    "\n",
    "def cost(x):\n",
    "    return x**2 + 5*np.sin(x)\n",
    "\n",
    "def myGD(x0, eta):\n",
    "    x = [x0]\n",
    "    for it in range(100):\n",
    "        Xnew = x[-1] - eta * grad(x[-1])\n",
    "        if abs(grad(Xnew)) < 1e-3:  # Dừng nếu gradient nhỏ hơn ngưỡng 1e-3\n",
    "            break\n",
    "        x.append(Xnew)\n",
    "    return (x, it)\n",
    "\n",
    "# Sửa lại tên hàm cho đúng\n",
    "(x1, it1) = myGD(-5, .1)\n",
    "(x2, it2) = myGD(5, .1)\n",
    "\n",
    "# In ra kết quả với hàm chi phí (cost)\n",
    "print('Solution x1 = %f, cost = %f, after %d iterations' % (x1[-1], cost(x1[-1]), it1))\n",
    "print('Solution x2 = %f, cost = %f, after %d iterations' % (x2[-1], cost(x2[-1]), it2))\n",
    "\"\"\"\n",
    "Hàm grad(x): Tính gradient (đạo hàm) của hàm mục tiêu. Hàm trả về kết quả theo công thức 2*x + 5*np.cos(x).\n",
    "\n",
    "Hàm cost(x): Tính giá trị của hàm mục tiêu với công thức x**2 + 5*np.sin(x). Đây là hàm để tối ưu hóa.\n",
    "\n",
    "Hàm myGD(x0, eta):\n",
    "\n",
    "x0: Giá trị khởi tạo của biến.\n",
    "eta: Tốc độ học (learning rate) bước nhảy trong mỗi lần lặp.\n",
    "Hàm thực hiện tối ưu hóa dựa trên Gradient Descent (GD). Nó lặp qua tối đa 100 lần, tính cập nhật giá trị mới của x bằng cách trừ đi gradient đã nhân với tốc độ học. Nếu gradient nhỏ hơn một ngưỡng rất nhỏ (1e-3), vòng lặp sẽ dừng lại.\n",
    "Kết quả là danh sách các giá trị x đã tìm thấy và số lần lặp (iteration).\n",
    "Phần kết quả: In ra nghiệm tìm được của hàm x1 và x2, cùng với giá trị hàm chi phí và số lần lặp cho mỗi nghiệm.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b9ba7-0ceb-4f25-9dca-857778198ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbf212a-2da4-4352-97bd-81691b9987e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
